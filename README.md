# MeMAD multimodal content analysis: collection of tools and libraries

This repository contains a joint collection of libraries and tools for
multi-modal content analysis from AALTO, EURECOM and INA. The majority
of the tools included have been created before the MeMAD project, but
have been further developed and will continue to be developed during
the project.

The collection consists of the following submodules:

## AALTO
 * PicSOM: <https://github.com/aalto-cbir/PicSOM>
 * DeepCaption: <https://github.com/aalto-cbir/DeepCaption>
 * Visual storytelling: <https://github.com/aalto-cbir/visual-storytelling>
 * Speech recognition training scripts for Finnish: <https://github.com/psmit/char-fin-2017>
 * Speaker-aware training: <https://github.com/Gastron/espnet-old-speaker-aware>
 * SphereDiar: <https://github.com/Livefull/SphereDiar>
 * Multimodal ASR: <https://github.com/aalto-speech/avsr>
 * Audio event classification: <https://github.com/MeMAD-project/AudioTagger>
 * Multi-modal image caption translation: <https://github.com/Waino/OpenNMT-py/tree/develop_mmod>
 * Statistical tools for caption dataset analysis: <https://github.com/MeMAD-project/statistical-tools>
 
## EURECOM
 * Face Celebrity Recognition: <https://github.com/D2KLab/Face-Celebrity-Recognition.git>

## INA
 * inaSpeechSegmenter: <https://github.com/ina-foss/inaSpeechSegmenter>

## University of Helsinki
 * [Research output from work package 4](https://github.com/MeMAD-project/workpackages)
 * [Subtitle translation](https://github.com/MeMAD-project/subtitle-translation)
 * [Tools for converting and aligning subtitles](https://github.com/MeMAD-project/subalign)
 * [Speech translation](https://github.com/MeMAD-project/speech-translation)
 * [Discourse-aware machine transltion](https://github.com/MeMAD-project/doclevel-translation)
 * [Document-level machine translation benchmarks](https://zenodo.org/record/3525366)
 * [Cross-lingual content retrieval](https://github.com/MeMAD-project/cross-lingual-retrieval)
 * [OpenSubtitles2018: a large collection of aligned movie subtitles](http://opus.nlpl.eu/OpenSubtitles-v2018.php)
 * [TED2020: Aligned TedTalk subtitles](http://opus.nlpl.eu/TED2020.php)
 * [QED: Aligned subtitles of educational videos](http://opus.nlpl.eu/QED.php)
 * [OPUS-MT: MT servers and pre-trained translation models](https://github.com/MeMAD-project/Opus-MT)
 * [OPUS-MT-train: MT training procedures and pipelines](https://github.com/MeMAD-project/OPUS-MT-train)
 * [OPUS-eval: A collection of MT benchmarks](https://github.com/MeMAD-project/OPUS-MT-eval)
 * [The Tatoeba MT Challenge: Multilingual data sets and benchmarks for machine translation](https://github.com//MeMAD-project/Tatoeba-Challenge)
 * [OPUS-CAT: MT plugins for professional translators](https://github.com/MeMAD-project/OPUS-CAT)
 * [OPUS-translator: Web interface for machine translation](https://github.com/MeMAD-project/OPUS-translator)
  

![EU emblem](euflag.png)                         | MeMAD project has received funding from the European Unionâ€™s Horizon 2020 research and innovation programme under grant agreement No 780069. This document has been produced by the MeMAD project. The content in this document represents the views of the authors, and the European Commission has no liability in respect of the content.
------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

